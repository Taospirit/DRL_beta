env: Pendulum-v0 
state_space: 3, action_space: 1
episodes: 1000, max_step: 100
===============================
lr: 0.01 
eps: 1.1920928955078125e-07 
next_state: None 
actor_learn_freq: 1 
target_update_freq: 5 
_gamma: 0.99 
_target: True 
_update_iteration: 10 
_sync_cnt: 0 
_learn_cnt: 0 
_verbose: False 
_buffer: <drl.buffer.ReplayBuffer object at 0x7f7086686f98> 
_batch_size: 300 
device: cuda 
actor_eval: ActorDPG(
  (net): Sequential(
    (0): Linear(in_features=3, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=32, bias=True)
    (3): ReLU()
    (4): Linear(in_features=32, out_features=1, bias=True)
    (5): Tanh()
  )
) 
critic_eval: Critic(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=32, bias=True)
    (3): ReLU()
    (4): Linear(in_features=32, out_features=1, bias=True)
  )
) 
actor_eval_optim: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 0
) 
critic_eval_optim: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 0
) 
actor_target: ActorDPG(
  (net): Sequential(
    (0): Linear(in_features=3, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=32, bias=True)
    (3): ReLU()
    (4): Linear(in_features=32, out_features=1, bias=True)
    (5): Tanh()
  )
) 
critic_target: Critic(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=32, bias=True)
    (3): ReLU()
    (4): Linear(in_features=32, out_features=1, bias=True)
  )
) 
criterion: MSELoss() 
